{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadowing in ERM (Learning Halfspaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "def tic():\n",
    "    #Homemade version of matlab tic and toc functions\n",
    "    import time\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "def toc():\n",
    "    import time\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        print(str(time.time() - startTime_for_tictoc))\n",
    "    else:\n",
    "        print('Toc: start time not set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-89974041a09b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mnit_ode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2e4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mnit_algo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnit_ode\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mratio_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0msubsampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "details0 = '5_runs_GD_visc_0.3_MNIST_step_1_ODE_RK4_withstep_0.1_nit_20000_reg_0.005'\n",
    "\n",
    "\n",
    "#select GD or HB\n",
    "algorithm = 'GD'\n",
    "\n",
    "#simulation of corresponding ODE : \n",
    "#possible are EULER, RK2, RK4 for GD and VERLET for HB\n",
    "simulation = 'RK4'\n",
    "h_ode = 0.1 #simulation stepsize\n",
    "alpha = 0.3 #viscosity (in case HB)\n",
    "beta_ode = 1-alpha*h_ode #momentum parameter for VERLET simulation\n",
    "\n",
    "#algorithm parameters\n",
    "h_algo = 0.2\n",
    "beta_algo = 1-alpha*h_algo #momentum parameter for HB\n",
    "\n",
    "#link for time variables\n",
    "ratio_h = round(h_algo/h_ode)\n",
    "\n",
    "#dataset : possible are IRIS(toy), MNIST, Fashion-MNIST, CIFAR10\n",
    "dataset = 'Fashion-MNIST'\n",
    "number_datapoints = 10000\n",
    "\n",
    "#number of iterations\n",
    "nit_ode = int(2e4)\n",
    "nit_algo = int(nit_ode/ratio_h)\n",
    "runs = 5\n",
    "subsampling = 1000\n",
    "\n",
    "#parameters model\n",
    "lambda_reg = 0.005\n",
    "add_bias = 1\n",
    "normalize = 0 \n",
    "\n",
    "seeds = [710, 28, 56, 80, 65]\n",
    "# for saving\n",
    "details = str(runs)+'_runs_'+algorithm+'_visc_'+str(alpha)+'_'+dataset+'_step_'+str(h_algo)+'_ODE_'+simulation+'_withstep_'+str(h_ode)+'_nit_'+str(nit_ode)+'_reg_'+str(lambda_reg)\n",
    "print(details)\n",
    "\n",
    "#43 is a lucky seed for 100 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if   dataset == 'IRIS': #toy example\n",
    "    \n",
    "    iris = datasets.load_iris()\n",
    "    iris.target[iris.target != 0] = -1\n",
    "    iris.target[iris.target == 0] = 1\n",
    "    iris.target = iris.target.reshape(-1,1)\n",
    "    Z = torch.from_numpy(iris.data[:, :2])\n",
    "    y = torch.from_numpy(iris.target).double()\n",
    "    \n",
    "else:\n",
    "    if dataset == 'MNIST':\n",
    "        train_dataset = torchvision.datasets.MNIST(root='data-MNIST', train=True, transform=transforms.ToTensor(),download=True)\n",
    "        dim_img = 28\n",
    "    elif dataset == 'Fashion-MNIST':\n",
    "        train_dataset=torchvision.datasets.FashionMNIST(root='data-Fashion-MNIST', train=True, transform=transforms.ToTensor(), download=True)        \n",
    "        dim_img = 28\n",
    "    elif dataset == 'CIFAR10':\n",
    "        trainTransform  = torchvision.transforms.Compose([torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "                                    torchvision.transforms.ToTensor(), \n",
    "                                    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        train_dataset = torchvision.datasets.CIFAR10(root='data-CIFAR', train=True, transform=trainTransform,download=True)\n",
    "        dim_img = 32\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=int(len(train_dataset)), shuffle=False)\n",
    "    images, labels = next(iter(train_loader))\n",
    "    images = torch.squeeze(images)\n",
    "    images = images.reshape(-1, dim_img*dim_img).double()\n",
    "    digit1 = 3 #cats in CIFAR\n",
    "    digit2 = 5 #dogs in CIFAR\n",
    "    \n",
    "    if number_datapoints>=images.shape[0]:\n",
    "        number_datapoints = images.shape[0]\n",
    "    nd = int(np.floor(number_datapoints/2))\n",
    "    \n",
    "    #first class\n",
    "    mask1 = (labels == digit1)\n",
    "    images1 = images[mask1]\n",
    "    labels1 = labels[mask1]\n",
    "    Z1 = images1[range(nd),:]\n",
    "    y1 = labels1[range(nd)].double().reshape(-1,1)\n",
    "    y1[:] = -1\n",
    "    \n",
    "    #second class\n",
    "    mask2 = (labels == digit2)\n",
    "    images2 = images[mask2]\n",
    "    labels2 = labels[mask2]\n",
    "    Z2 = images2[range(nd),:]\n",
    "    y2 = labels2[range(nd)].double().reshape(-1,1)\n",
    "    y2[:] = 1\n",
    "    \n",
    "    #merging to get dataset\n",
    "    Z=torch.cat((Z1,Z2), 0)\n",
    "    y=torch.cat((y1,y2), 0)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(Z1[0,:].reshape(dim_img,dim_img),cmap='gray')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(Z2[0,:].reshape(dim_img,dim_img),cmap='gray')    \n",
    "    plt.show()\n",
    "    \n",
    "if normalize:\n",
    "    m=torch.mean(Z,dim=0)\n",
    "    s=torch.std(Z,dim=0)\n",
    "    for j in range(Z.shape[1]):\n",
    "        if s[j]==0:\n",
    "            Z[:,j] = 0\n",
    "        else:\n",
    "            Z[:,j] = (Z[:,j]-m[j])/s[j]\n",
    "    \n",
    "if add_bias:\n",
    "    Z=torch.cat((Z,1+0*y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost function (Sigmoid loss)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "   l = lambda_reg*x.t()@x + torch.sigmoid(y*(Z@x)).mean()\n",
    "   return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outer loop variables for comparison\n",
    "perc_res_all = np.zeros([nit_algo,runs])\n",
    "accuracy_algo_all = np.zeros([nit_algo,runs])\n",
    "accuracy_ode_all = np.zeros([nit_algo,runs])\n",
    "loss_hist_algo_all = np.zeros([nit_algo,runs])\n",
    "loss_hist_ode_all = np.zeros([nit_algo,runs])\n",
    "\n",
    "tic()\n",
    "for e in range(runs):\n",
    "    seed = seeds[e]\n",
    "    print(seed)\n",
    "    \n",
    "    #BEGIN ALGORITHM \n",
    "    torch.manual_seed(seed)\n",
    "    x = torch.randn([Z.shape[1],1],requires_grad=True)\n",
    "    x_hist_algo = np.zeros([Z.shape[1],nit_algo])\n",
    "    loss_hist_algo = np.zeros([nit_algo,])\n",
    "    \n",
    "    for k in range(nit_algo):\n",
    "        #computing the loss\n",
    "        l =  loss(x)\n",
    "        #saving to history\n",
    "        x_hist_algo[:,k] =  np.squeeze(x.data.numpy())\n",
    "        loss_hist_algo[k] = l.data.numpy()\n",
    "\n",
    "        #update for GD\n",
    "        if algorithm == 'GD':\n",
    "            l.backward() #backprop\n",
    "            with torch.no_grad():\n",
    "                x -= h_algo*x.grad\n",
    "            x.grad.data.fill_(0)\n",
    "\n",
    "        #update for HB\n",
    "        elif algorithm == 'HB':\n",
    "            l.backward() #backprop\n",
    "            with torch.no_grad():\n",
    "                if k==0:\n",
    "                    momentum = 0\n",
    "                else:\n",
    "                    momentum = torch.from_numpy(x_hist_algo[:,k]-x_hist_algo[:,k-1]).reshape(-1,1)\n",
    "                x += -(h_algo*h_algo)*x.grad + beta_algo*momentum \n",
    "            x.grad.data.fill_(0)\n",
    "\n",
    "        else: sys.exit('SELECT A VALID ALGORITHM (GD or HB)!!')\n",
    "        #END ALGORITHM\n",
    "\n",
    "    #BEGIN ODE\n",
    "    torch.manual_seed(seed)\n",
    "    x = torch.randn([Z.shape[1],1],requires_grad=True)\n",
    "    y1 = torch.randn([Z.shape[1],1],requires_grad=True) #needed for Runge Kutta\n",
    "    y2 = torch.randn([Z.shape[1],1],requires_grad=True) #needed for Runge Kutta\n",
    "    y3 = torch.randn([Z.shape[1],1],requires_grad=True) #needed for Runge Kutta\n",
    "    x_hist_ode = np.zeros([Z.shape[1],nit_ode])\n",
    "    loss_hist_ode = np.zeros([nit_ode,])\n",
    "    for k in range(nit_ode):\n",
    "        #computing the loss\n",
    "        l =  loss(x)\n",
    "\n",
    "        #saving to history\n",
    "        x_hist_ode[:,k] =  np.squeeze(x.data.numpy())\n",
    "        loss_hist_ode[k] = l.data.numpy()\n",
    "\n",
    "        #performing numerical integration\n",
    "        if algorithm == 'GD':\n",
    "            if simulation == 'EULER':\n",
    "                #Euler Method = GD (1 backpropagation)\n",
    "                #     to simulate dx/dt = f(x)     \n",
    "                #     k1 = h * f(x)\n",
    "                #     x = x + k1\n",
    "                l.backward() \n",
    "                with torch.no_grad(): \n",
    "                    k1 = -h_ode*x.grad\n",
    "                    x += k1\n",
    "                    x.grad.data.fill_(0)    \n",
    "\n",
    "\n",
    "            elif simulation == 'RK2':\n",
    "                #second order Runge Kutta (2 backpropagations)\n",
    "                #     to simulate dx/dt = f(x)\n",
    "                #     k1 = h * f(x)\n",
    "                #     k2 = h * f(x + k1)\n",
    "                #     x = x + (k1 + k2) / 2\n",
    "                l.backward() \n",
    "                with torch.no_grad(): \n",
    "                    k1 = -h_ode*x.grad\n",
    "                    y1 = x+k1\n",
    "                    x.grad.data.fill_(0)\n",
    "                    y1.requires_grad=True\n",
    "                l =  loss(y1)  \n",
    "                l.backward() \n",
    "                with torch.no_grad():\n",
    "                    k2 = -h_ode*y1.grad\n",
    "                    x += (k1 + k2) / 2\n",
    "                    y1.grad.data.fill_(0)\n",
    "\n",
    "\n",
    "            elif simulation == 'RK4':\n",
    "                #forth order Runge Kutta (4 backpropagations)\n",
    "                #     to simulate dx/dt = f(x)     \n",
    "                #     k1 = h * f(x)\n",
    "                #     k2 = h * f(x + 0.5 * k1)\n",
    "                #     k3 = h * f(x + 0.5 * k2)\n",
    "                #     k4 = h * f(x + k3)\n",
    "                #     x = x + (k1 + k2 + k2 + k3 + k3 + k4) / 6\n",
    "                l.backward() \n",
    "                with torch.no_grad(): \n",
    "                    k1 = -h_ode*x.grad\n",
    "                    y1 = x+0.5*k1\n",
    "                    x.grad.data.fill_(0)\n",
    "                    y1.requires_grad=True\n",
    "                l =  loss(y1)\n",
    "                l.backward()\n",
    "                with torch.no_grad():\n",
    "                    k2 = -h_ode*y1.grad\n",
    "                    y2 = x+0.5*k2\n",
    "                    y1.grad.data.fill_(0)\n",
    "                    y2.requires_grad=True\n",
    "                l =  loss(y2)\n",
    "                l.backward() \n",
    "                with torch.no_grad():\n",
    "                    k3 = -h_ode*y2.grad\n",
    "                    y3 = x+k3\n",
    "                    y2.grad.data.fill_(0)\n",
    "                    y3.requires_grad=True\n",
    "                l =  loss(y3)  \n",
    "                l.backward() \n",
    "                with torch.no_grad():\n",
    "                    k4 = -h_ode*y3.grad\n",
    "                    x += (k1 + k2 + k2 + k3 + k3 + k4) / 6\n",
    "                    y3.grad.data.fill_(0)\n",
    "\n",
    "            else: sys.exit('SELECT A VALID INTEGRATOR (EULER, RK2, RK4)!!')\n",
    "\n",
    "\n",
    "        elif algorithm == 'HB':\n",
    "            if simulation == 'VERLET':\n",
    "                l.backward() #backprop\n",
    "                with torch.no_grad():\n",
    "                    if k==0:\n",
    "                        momentum = 0\n",
    "                    else:\n",
    "                        momentum = torch.from_numpy(x_hist_ode[:,k]-x_hist_ode[:,k-1]).reshape(-1,1)\n",
    "                    x += -(h_ode*h_ode)*x.grad + beta_ode*momentum \n",
    "                    x.grad.data.fill_(0)\n",
    "            else: sys.exit('SELECT A VALID INTEGRATOR (ONLY VERLET AVAILABLE)!!')\n",
    "\n",
    "        else: sys.exit('SELECT A VALID ALGORITHM (GD or HB)!!')\n",
    "        #END ODE\n",
    "\n",
    "    x_hist_ode = x_hist_ode[:,::ratio_h]\n",
    "    loss_hist_ode = loss_hist_ode[::ratio_h]\n",
    "\n",
    "    #computing shadowing radius\n",
    "    res = x_hist_algo-x_hist_ode\n",
    "    perc_res = 100*np.linalg.norm(res, axis=0)/np.linalg.norm(x_hist_ode, axis=0)\n",
    "\n",
    "    #computing accuracy ode\n",
    "    ZZ = Z.numpy()\n",
    "    yy = np.squeeze(y.numpy())\n",
    "    prediction_ode = ZZ@x_hist_ode\n",
    "    accuracy_ode = 0*prediction_ode[0,:]\n",
    "    for j in range(len(accuracy_ode)):\n",
    "        idxpos = prediction_ode[:,j]>=0\n",
    "        idxneg = prediction_ode[:,j]<0\n",
    "        prediction_ode[idxpos,j] = 1\n",
    "        prediction_ode[idxneg,j] = -1        \n",
    "        accuracy_ode[j] = 100*(np.sum(np.abs(prediction_ode[:,j] - yy))/2)/prediction_ode.shape[0]\n",
    "\n",
    "    #computing accuracy algorithm\n",
    "    prediction_algo = ZZ@x_hist_algo\n",
    "    accuracy_algo = 0*prediction_algo[0,:]\n",
    "    for j in range(len(accuracy_algo)):\n",
    "        idxpos = prediction_algo[:,j]>=0\n",
    "        idxneg = prediction_algo[:,j]<0\n",
    "        prediction_algo[idxpos,j] = 1\n",
    "        prediction_algo[idxneg,j] = -1        \n",
    "        accuracy_algo[j] = 100*(np.sum(np.abs(prediction_algo[:,j] - yy))/2)/prediction_algo.shape[0]\n",
    "        \n",
    "    ########## END OF SINGLE COMPUTATION, SAVING ########\n",
    "    perc_res_all[:,e] = perc_res\n",
    "    accuracy_algo_all[:,e] = accuracy_algo\n",
    "    accuracy_ode_all[:,e] = accuracy_ode\n",
    "    loss_hist_algo_all[:,e] = loss_hist_algo\n",
    "    loss_hist_ode_all[:,e] = loss_hist_ode\n",
    "toc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0=plt.figure()\n",
    "#plt.plot(loss_hist_algo_all)\n",
    "plt.plot(loss_hist_ode_all)\n",
    "plt.xlabel('$k$',fontsize=10)\n",
    "plt.ylabel('sigmoid loss',fontsize=10)\n",
    "fig0.savefig('loss.eps', format='eps', dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "fig1=plt.figure()\n",
    "plt.plot(perc_res_all)\n",
    "plt.xlabel('$k$',fontsize=10)\n",
    "plt.ylabel('$||x_k-y_k||$',fontsize=10)\n",
    "plt.ylim(0,1.05*np.max(perc_res_all))\n",
    "plt.gca().set_yticklabels(['{:.3f}%'.format(x) for x in plt.gca().get_yticks()]) \n",
    "fig1.savefig('shad.eps', format='eps', dpi=1000)\n",
    "plt.show()\n",
    "\n",
    "fig2=plt.figure()\n",
    "#plt.plot(accuracy_algo_all)\n",
    "plt.plot(accuracy_ode_all)\n",
    "plt.xlabel('$k$',fontsize=10)\n",
    "plt.ylabel('$accuracy$',fontsize=10)\n",
    "plt.ylim(0,100)\n",
    "plt.grid(True)\n",
    "fig2.savefig('acc.eps', format='eps', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(loss_hist_ode_all,'results/loss_'+details+'.pt')\n",
    "torch.save(perc_res_all,'results/perc_'+details+'.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
